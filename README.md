# Transformer 2
Radi Akbar
Personal Project

## Introduction
The world of NLP is developing at a rapid space. There has been so many updates to the original Transformer architecture (Vaswani, et al, 2017), that its hard keeping track of all the updates that happened. This project aims to showcase some of the recent developments in the Transformer architecture through tweaking the sequence-to-sequence model and compare the performance of the original architecture against the updated one.

## Dependencies
* Python 3.9.16
* torch 
* tqdm
* einops

## Architectural Updates
### Rotary Positional Embedding
### SwiGLU
### RMSNorm
### Multihead - Attention

## Training
### Optimizer
### Learning Rate Schedule
### Configuration
